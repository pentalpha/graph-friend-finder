{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### API >> https://facebook-sdk.readthedocs.io/en/latest/api.html\n",
    "### Get access token >> https://developers.facebook.com/tools/explorer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!pip install facebook-sdk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#START\n",
    "#this is my token (Pitágoras)\n",
    "token = \"EAACEdEose0cBAH2ihx79uCdJ2GdqTsDgBJAy1LJZBTKaTyg4w7Tl9nI3n43cIaJAHrEYK0jJ5QtWiIgTXieVJ56ze0872vYGKcsXZCi1aygDXUiJ3Fo3c6VLBa0OukO5pLZCW1RTjhAjkGzA2OxSDZACKdPxDc1LNrk2mzkeQ9fD0pjeHVOCH3zqgVxI3ysZD\"\n",
    " \n",
    "#ID of \"Pitagoras\"\n",
    "otherPersonID = \"10208935375967359\"\n",
    "raiID = \"739425496128479\"\n",
    "anaID = \"765831436837419\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict as ddict\n",
    "import facebook as fb\n",
    "import requests as req\n",
    "%pylab inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Gets the categories of pages liked by the user\n",
    "#   token       user access token\n",
    "#   return      dictionary of categories found, each with it's own array of pages\n",
    "#       {c1 : [page1, page2, page3...], c2 : [page4, page5], c3 : [page6]...}\n",
    "def getCats(token):\n",
    "    cats = ddict(str)\n",
    "    graph = fb.GraphAPI(access_token=token, version='2.7')\n",
    "    resource = graph.get_object(\"me/likes?fields=name,category\")\n",
    "    #print(resource)\n",
    "    cats = dict()\n",
    "    while(True):\n",
    "        for page in resource['data']:\n",
    "            cat = page['category']\n",
    "            if not(cat in cats):\n",
    "                cats[cat] = []\n",
    "            cats[cat].append(page['id'])\n",
    "        # Attempt to make a request to the next page of data, if it exists.\n",
    "        try:\n",
    "            resource=req.get(resource['paging']['next']).json()\n",
    "        except KeyError:\n",
    "            #print(\"Finished getting pages\")\n",
    "            # When there are no more pages (['paging']['next']), break from the\n",
    "            # loop and end the script.\n",
    "            break\n",
    "    return cats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Classifies pages by categories, the category with more pages gets 1.0\n",
    "#and the smaller categories get smaller and smaller classifications...\n",
    "#   cats        dictionary of categories found, each with it's own array of pages\n",
    "#       {c1 : [page1, page2, page3...], c2 : [page4, page5], c3 : [page6]...}\n",
    "#   return      dictionary of pages, each with it's own classification (0.0 < classification <= 1.0)\n",
    "#       {page1 : x, page2 : x, page4 : y, page6 : z}\n",
    "def classify(cats):\n",
    "    classification = dict()\n",
    "    biggestCat = \"\"\n",
    "    biggestCatLen = 0\n",
    "    for cat, pages in cats.items():\n",
    "        if(len(pages) > biggestCatLen):\n",
    "            biggestCat = cat\n",
    "            biggestCatLen = len(pages)\n",
    "    for cat, pages in cats.items():\n",
    "        classify = len(pages)/biggestCatLen\n",
    "        for i in pages:\n",
    "            classification[i] = classify\n",
    "\n",
    "    return classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import operator\n",
    "normalWeigth = [25, 18, 15, 12, 10, 8, 6, 4, 2, 1]\n",
    "\n",
    "# Normalizes the value previously attributed to each category of page\n",
    "# Acording to the array 'normalWeigth', the category with the highest value\n",
    "# receives a value of 25, the 2º 18, the 3º 15 and soo on... starting from the 10º,\n",
    "# all categories receive a value of 1.\n",
    "# c        dictionary of categories, each with its given value\n",
    "# return   dictionary of categories, normalized\n",
    "def norm(c):\n",
    "    tuples = list()\n",
    "    sortedByVal = sorted(c.items(), key=operator.itemgetter(1))\n",
    "    \n",
    "    for i in range(0, len(sortedByVal)-11):\n",
    "        tup = sortedByVal[i]\n",
    "        tuples.append((tup[0], 1))\n",
    "    count = 9\n",
    "    for i in range(len(sortedByVal)-9, len(sortedByVal)):\n",
    "        a = normalWeigth[count]\n",
    "        if(i > 0):\n",
    "            tup = sortedByVal[i]\n",
    "            tuples.append((tup[0], a))\n",
    "            count = count - 1\n",
    "    newDict = dict()\n",
    "    for t in tuples:\n",
    "        newDict[t[0]] = t[1]\n",
    "    return newDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Get the pages that a given user liked\n",
    "#   user    id of a user\n",
    "#   token   access token\n",
    "#   return  array of page IDs\n",
    "#       [page1, page2, page3...]\n",
    "def getPagesFromUser(user, token):\n",
    "    pages = list()\n",
    "    try:\n",
    "        graph = fb.GraphAPI(access_token=token, version='2.7')\n",
    "        resource = graph.get_object(user+\"?fields=likes{category,name}\")[\"likes\"]\n",
    "    except:\n",
    "        return pages\n",
    "    cats = dict()\n",
    "    while(True):\n",
    "        for page in resource['data']:\n",
    "            pages.append(page['id'])\n",
    "        # Attempt to make a request to the next page of data, if it exists.\n",
    "        try:\n",
    "            resource=req.get(resource['paging']['next']).json()\n",
    "        except KeyError:\n",
    "            #print(\"Finished getting pages from user \" + user)\n",
    "            # When there are no more pages (['paging']['next']), break from the\n",
    "            # loop and end the script.\n",
    "            break\n",
    "    return pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Calculates the similarity between a user \n",
    "#and the current user we are searching friends for\n",
    "#user              The ID of the other user\n",
    "#classification    The classification of each page \n",
    "#                  the current user likes\n",
    "#token             Current user access token\n",
    "#return            The similarity rate, in float\n",
    "def similarityWith(user, classification, token):\n",
    "    otherUserPages = getPagesFromUser(user, token)\n",
    "    rate = 0.0\n",
    "    for page in otherUserPages:\n",
    "        if page in classification:\n",
    "            rate += classification[page]\n",
    "    return rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Get only the friends of the user\n",
    "#user     The user to search friends\n",
    "#token    Access token\n",
    "#return   The user's friends:\n",
    "#            dict { id1 : friend_name1, id2 : friend_name2 }\n",
    "def getFriends(user, token):\n",
    "    graph = fb.GraphAPI(access_token=token, version='2.7')\n",
    "    resource = graph.get_object(\"/\" + user + \"/friends\")\n",
    "    friends = dict()\n",
    "    while(True):\n",
    "        for friend in resource['data']:\n",
    "            friends[friend['id']] = friend['name']\n",
    "            # Attempt to make a request to the next page of data, if it exists.\n",
    "        try:\n",
    "            resource=req.get(resource['paging']['next']).json()\n",
    "        except KeyError:\n",
    "            #print(\"Finished getting friends from user \" + user)\n",
    "            # When there are no more pages (['paging']['next']), break from the\n",
    "            # loop and end the script.\n",
    "            break\n",
    "\n",
    "    return friends\n",
    "\n",
    "#Get the friends of the user and friends of friends too\n",
    "#user     The user where the search for users starts\n",
    "#token    Access token\n",
    "#return   A dict of users:\n",
    "#            dict { id1 : friend_name1, id2 : friend_name2 }\n",
    "def getUsersDict(user, token):\n",
    "    friends = getFriends(user, token)\n",
    "    ffriends = dict() \n",
    "    listFFriends = []\n",
    "    for idFriend in friends: \n",
    "        friendsOfFriends = getFriends(idFriend, token)\n",
    "        for idUser, name in friendsOfFriends.items():\n",
    "            if not idUser in ffriends and not idUser in friends:\n",
    "                ffriends[idUser] = name\n",
    "                listFFriends.append([idUser, name])\n",
    "    \n",
    "    count = 0\n",
    "    while len(listFFriends) < 400 and len(listFFriends) > count:\n",
    "        curruentID = listFFriends[count][0]\n",
    "        friendsCurrentID = getFriends(curruentID, token)\n",
    "        for idUser, name in friendsOfFriends.items():\n",
    "            if not idUser in ffriends:\n",
    "                ffriends[idUser] = name\n",
    "                listFFriends.append([idUser, name])\n",
    "        count+=1\n",
    "    return ffriends"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import threading\n",
    "threadsLimit = 32\n",
    "lock = threading.Lock()\n",
    "\n",
    "#This function is supposed to be used as a thread to calculate users similarities\n",
    "#When started, informs the counter that a new thread is running, and at the end,\n",
    "#informs the counter that its no longer running.\n",
    "#It uses a lock for safe concurrency.\n",
    "#When the similarity ratio has been calculated, stores it on similarityDict\n",
    "def similarityCalcThread(key, classf, token, similarityDict, threadsRunning):\n",
    "    lock.acquire()\n",
    "    threadsRunning = threadsRunning + 1\n",
    "    lock.release()\n",
    "    \n",
    "    ratio = similarityWith(key, classf, token)\n",
    "    similarityDict[key] = ratio\n",
    "    \n",
    "    lock.acquire()\n",
    "    threadsRunning = threadsRunning - 1\n",
    "    lock.release()\n",
    "\n",
    "#Uses paralell processing to calculate the similarity of many users at the same time\n",
    "#Waits for all the similarity's calculating threads to end\n",
    "#Has a limit of running threads\n",
    "def getSimilarityDict(friends, classf, token):\n",
    "    similarityDict = dict()\n",
    "    threadsRunning = 0\n",
    "    threads = list()\n",
    "    for key, value in friends.items():\n",
    "        while(threadsRunning > threadsLimit):\n",
    "            i = 1\n",
    "            #do nothing\n",
    "        t = threading.Thread(target=similarityCalcThread,\n",
    "                             args = (key, classf, token, similarityDict, threadsRunning))\n",
    "        t.daemon = False\n",
    "        threads.append(t)\n",
    "        t.start()\n",
    "    \n",
    "    #make sure ALL threads have ended\n",
    "    for x in threads:\n",
    "         x.join()\n",
    "    return similarityDict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Time to use the functions created above to do cool stuff\n",
    "### The major problem with our proposal was that, since version 2.0, the Graph API only lets you access friend's likes. This eliminates the possibility of using like's similarity to compare two users that are not friends.\n",
    "### So, for demonstration porpuses, we are only classifying the friends of the user."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "users = getFriends('me', token)\n",
    "#users = getUsersDict('me', token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We get the page categories, classify and normalize them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cats = getCats(token)\n",
    "classf = norm(classify(cats))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now its time for the heavy processing, calculating the similarity with the other users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "simDict = getSimilarityDict(users, classf, token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pandas time! We got all the data, let's build the DataFrame and sort it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   similarity           user-name\n",
      "user-id                                          \n",
      "765831436837419          71.0        Ana Caroline\n",
      "998100796886441          57.0         Ikaro Souza\n",
      "739425496128479          42.0           Rai Vitor\n",
      "773541116034720          23.0        Murilo Bento\n",
      "862034200484130          22.0        Ivan Alisson\n",
      "100002541369604          13.0  Aryan Dantas Gomes\n",
      "100002211536120          12.0       Thuize Thainá\n",
      "967446623299200           8.0         Paula Lopes\n",
      "644347655699194           7.0       Kamilla Sâmia\n",
      "653306944773428           5.0      Danilo Martins\n",
      "10208935375967359         3.0     Gilberto Soares\n",
      "100000888984231           0.0        Luan Fonseca\n",
      "607827396001201           0.0     Inacio Medeiros\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "#Making a series out of the similarity dict\n",
    "s = pd.Series(simDict, name='similarity')\n",
    "s.index.name = 'user-id'\n",
    "s.reset_index()\n",
    "#Turn it into a dataframe and sort it\n",
    "df_similarity = s.to_frame()\n",
    "sortedDF = df_similarity.sort_values([\"similarity\"], ascending=[False])\n",
    "\n",
    "#Now, append the user names, because we are humans\n",
    "usersSeries = pd.Series(users, name='user-name')\n",
    "usersSeries.index.name = 'user-id'\n",
    "usersSeries.reset_index()\n",
    "usersDF = usersSeries.to_frame()\n",
    "sortedDF['user-name'] = usersDF['user-name']\n",
    "\n",
    "#The final step\n",
    "print (sortedDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
